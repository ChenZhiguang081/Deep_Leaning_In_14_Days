1. 测试集与验证集的关系：
      模型训练的过程其实就是在求【参数】的过程，我们先假定某类【模型】（比如决策树模型），然后用【训练集】来训练，学习到对应的最优的【参数】。但是问题在于，我们没有办法保证我们假设的那个【模型】是最优的，我们极有可能假设错误对吧。那怎么办呢？有一个简单的解决方案就是我们假设一堆的模型，然后用【训练集】分别对这些模型来进行训练，学习到每一个【模型】中分别对应的参数——这是第一步，也就是【训练集】的任务。
      那么我们已经学习到了一堆的模型了，哪一个模型是最好的呢？这其实就是要来考察不同结构的模型在这些data上的优劣程度了。通常来说，我们用【超参数】来控制模型的结构（例如正则项系数、神经网络中隐层的节点个数，k值等）。那这个时候，我们就可以找一些数据来训练和学习我们具体的超参数了。用什么样的数据呢？直接用【训练集】肯定是不行的，因为我们现在的每一个模型都是用【训练集】来学习出来的，他们在【训练集】上的效果已经很好了，继续用它们来训练超参数不会有太大的效果，所以说我们就选择了使用【验证集】来选择这些超参数。这是第二步，也就是【验证集】的任务，我们也通常称之为【调参】。
      最后，当我们学习到了【参数】和【非参数】后，我们就确定了我们具体的模型结构，这个时候我们再用一些数据来测试这个模型在新的数据上的效果。因此，我们就不能够使用之前已经使用过的数据了，而要选择一个全新的数据集，这既是【测试集】。这个时候我们就要来看最后的结果怎么样，如果结果很好，那么说明一切顺利，但是如果结果很差，那问题出在哪里呢？其中可能的一个原因就是我们事先假定的那一类的【模型】（比如我们最先选择的决策树模型）并不是适合来分析这些数据，因此哪怕我们选择出了这一堆决策树模型中最好的一个（超参数的选择过程），它的效果依旧不怎么样。
      这里还有两个遗留的问题：
      （1）训练集、验证集和测试集的比例应该怎么去进行分配呢？
      传统上是6：2：2的比例，但是不同的情况下你的选择应当不同。这方面的研究也有很多，如果你想要知道我们在设置比例的时候应当参考那些东西，可以去看Isabelle Guyon的这篇论文：A scaling law for the validation-set training-set size ratio 。他的个人主页（http://www.clopinet.com/isabelle/）里也展示了他对于这个问题的研究。
      （2）训练集、验证集和测试集的数据是否可以有所重合？
      有些时候我们的数据太少了，又不想使用数据增强，那么训练集、验证集和测试集的数据是否可以有所重合呢？这方面的研究就更多了，各种交叉方法，感兴趣的话可以去看Filzmoser这一篇文章Repeated double cross validation

2. pytorch 的一些built-in函数
      本视频用了许多pytorch的函数，由于不是太了解pytorch内的函数，因此查询记录了一下。
      torch.ones()/torch.zeros()，与MATLAB的ones/zeros很接近。初始化生成
      均匀分布
      torch.rand(*sizes, out=None) → Tensor
      返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义。
      标准正态分布
      torch.randn(*sizes, out=None) → Tensor
      返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。
      torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵
      torch.mm(a, b)是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵
      torch.Tensor是一种包含单一数据类型元素的多维矩阵，定义了7种CPU tensor和8种GPU tensor类型。
      random.shuffle(a)：用于将一个列表中的元素打乱。shuffle() 是不能直接访问的，需要导入 random 模块，然后通过 random 静态对象调用该方法。
      backward()是pytorch中提供的函数，配套有require_grad：
      1.所有的tensor都有.requires_grad属性,可以设置这个属性.x = tensor.ones(2,4,requires_grad=True)
      2.如果想改变这个属性，就调用tensor.requires_grad_()方法：　　 x.requires_grad_(False)
